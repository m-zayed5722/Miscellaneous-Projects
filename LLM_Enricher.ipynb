{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZrSaz53PIAhILX7ZDKAhn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-zayed5722/Miscellaneous-Projects/blob/main/LLM_Enricher.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kqdl_E0ttZVF"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "import time\n",
        "from typing import Dict, List, Optional, Any, Union\n",
        "from dataclasses import dataclass\n",
        "import logging\n",
        "from src.schema import MenuItem, MenuSchema\n",
        "from pydantic import ValidationError\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class LLMConfig:\n",
        "    \"\"\"Configuration for LLM integration\"\"\"\n",
        "    model_name: str = \"llama2\"  # Default Ollama model\n",
        "    base_url: str = \"http://localhost:11434\"\n",
        "    timeout: int = 600  # 10 minutes timeout for large models\n",
        "    max_retries: int = 3\n",
        "    temperature: float = 0.1  # Low temperature for consistent structured output\n",
        "\n",
        "\n",
        "class OllamaClient:\n",
        "    \"\"\"Client for interacting with Ollama API\"\"\"\n",
        "\n",
        "    def __init__(self, config: LLMConfig = None):\n",
        "        self.config = config or LLMConfig()\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "    def is_available(self) -> bool:\n",
        "        \"\"\"Check if Ollama server is available\"\"\"\n",
        "        try:\n",
        "            response = requests.get(f\"{self.config.base_url}/api/tags\", timeout=5)\n",
        "            return response.status_code == 200\n",
        "        except Exception as e:\n",
        "            self.logger.warning(f\"Ollama not available: {e}\")\n",
        "            return False\n",
        "\n",
        "    def list_models(self) -> List[str]:\n",
        "        \"\"\"List available models\"\"\"\n",
        "        try:\n",
        "            response = requests.get(f\"{self.config.base_url}/api/tags\", timeout=5)\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                return [model['name'] for model in data.get('models', [])]\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Error listing models: {e}\")\n",
        "        return []\n",
        "\n",
        "    def generate(self, prompt: str, model: str = None) -> Optional[str]:\n",
        "        \"\"\"Generate text using Ollama\"\"\"\n",
        "        model = model or self.config.model_name\n",
        "\n",
        "        payload = {\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"stream\": False,\n",
        "            \"options\": {\n",
        "                \"temperature\": self.config.temperature\n",
        "            }\n",
        "        }\n",
        "\n",
        "        for attempt in range(self.config.max_retries):\n",
        "            try:\n",
        "                self.logger.info(f\"Sending request to {model} (timeout: {self.config.timeout}s)...\")\n",
        "                start_time = time.time()\n",
        "\n",
        "                response = requests.post(\n",
        "                    f\"{self.config.base_url}/api/generate\",\n",
        "                    json=payload,\n",
        "                    timeout=self.config.timeout\n",
        "                )\n",
        "\n",
        "                elapsed_time = time.time() - start_time\n",
        "                self.logger.info(f\"Response received in {elapsed_time:.2f} seconds\")\n",
        "\n",
        "                if response.status_code == 200:\n",
        "                    result = response.json()\n",
        "                    return result.get('response', '').strip()\n",
        "                else:\n",
        "                    self.logger.warning(f\"Ollama API error: {response.status_code} - {response.text}\")\n",
        "\n",
        "            except requests.exceptions.Timeout as e:\n",
        "                self.logger.warning(f\"Attempt {attempt + 1} timed out after {self.config.timeout}s: {e}\")\n",
        "                if attempt < self.config.max_retries - 1:\n",
        "                    self.logger.info(\"Retrying with exponential backoff...\")\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "            except Exception as e:\n",
        "                self.logger.warning(f\"Attempt {attempt + 1} failed: {e}\")\n",
        "                if attempt < self.config.max_retries - 1:\n",
        "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "class LLMEnricher:\n",
        "    \"\"\"LLM-based menu item enricher\"\"\"\n",
        "\n",
        "    def __init__(self, config: LLMConfig = None):\n",
        "        self.config = config or LLMConfig()\n",
        "        self.client = OllamaClient(config)\n",
        "        self.schema = MenuSchema()\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # System prompt template\n",
        "        self.system_prompt = \"\"\"You are an expert food categorization assistant. Your task is to analyze menu item names and return structured, clean information.\n",
        "\n"
      ]
    }
  ]
}